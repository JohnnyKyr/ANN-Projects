{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyKyr/CompIntel-Projects/blob/main/GAs2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmoCnFci59fb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import matplotlib.pyplot as plt\n",
        "from math import log2\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTdMg9za6J32"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataMatrix = None\n",
        "        self.labels = None\n",
        "        self.size = None\n",
        "\n",
        "    def getLexicalSize(self,file):\n",
        "        lexiconS = []\n",
        "        with open(file, \"r\") as fp:\n",
        "            for line in fp:\n",
        "                lexiconS.append(line.split()[0])\n",
        "        return  len(lexiconS)\n",
        "    def getLabels(self,data):\n",
        "        self.labels = []\n",
        "        with open(data, \"r\") as fp:\n",
        "            for line in fp:\n",
        "                self.labels.append([int(i) for i in line.split()])\n",
        "\n",
        "    def getData(self,data,lexicon):\n",
        "        bagof = []\n",
        "        bagofcsr =[]\n",
        "        with open(data,\"r\") as fp:\n",
        "            for line in fp:\n",
        "                vertex = [0 for i in range(self.getLexicalSize(lexicon))]\n",
        "                array = list(filter(lambda x : x[0] != '<' , line.split()))\n",
        "                for ele in array:\n",
        "                    vertex[int(ele)] +=1\n",
        "                self.size = len(vertex)\n",
        "                bagof.append(vertex)\n",
        "        return bagof\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyY2YfnF6Lce"
      },
      "outputs": [],
      "source": [
        "\n",
        "datalex = Parser()\n",
        "\n",
        "\n",
        "bagofwords = datalex.getData(\"/drive/MyDrive/ML/train-data.dat\",\"/drive/MyDrive/ML/vocabs.txt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrmR3k47r6d4"
      },
      "outputs": [],
      "source": [
        "from os import WCOREDUMP\n",
        "class Individual(object):\n",
        "    def __init__(self, chromosome):\n",
        "        self.chromosome = chromosome\n",
        "        self.fitness = self.calculate_fitness()\n",
        "\n",
        "    @classmethod\n",
        "    def create_genome(self):\n",
        "\n",
        "        genome = np.random.randint(0, 1, size=8520)\n",
        "        rand_ones = random.randint(0, 8519)\n",
        "        for _ in range(rand_ones):\n",
        "            genome[random.randint(0, 8519)] = 1\n",
        "        return genome\n",
        "\n",
        "    def calculate_fitness(self):\n",
        "        return 0\n",
        "\n",
        "\n",
        "def genome_rejection(genome: Individual):\n",
        "    # replace the non valid genome\n",
        "    # TODO recursion\n",
        "    while np.count_nonzero(genome) < 1000:\n",
        "        genome = Individual.create_genome()\n",
        "    return genome\n",
        "\n",
        "\n",
        "def repair_genome(genome: Individual):\n",
        "    nnz = np.count_nonzero(genome)\n",
        "    while nnz < 1000:\n",
        "        difference = 1000 - nnz\n",
        "        for _ in range(difference):\n",
        "            genome[random.randint(0, 8519)] = 1\n",
        "        nnz = np.count_nonzero(genome)\n",
        "    return genome\n",
        "def initialize_population(no_of_chromosomes):\n",
        "    population = []\n",
        "    for _ in range(no_of_chromosomes):\n",
        "        genome = Individual.create_genome()\n",
        "        # genome = genome_rejection(genome)\n",
        "        genome = repair_genome(genome)\n",
        "        population.append(genome)\n",
        "\n",
        "    return np.array(population)\n",
        "\n",
        "def fitnessFunc():\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tf_idf(bagofwords):\n",
        "  count=0\n",
        "  N=len(bagofwords)\n",
        "  n = len(bagofwords[0])\n",
        "  tfbag = []\n",
        "  \n",
        "  textDCount = np.count_nonzero(bagofwords,axis=0)  #word count of the text body\n",
        "  worddcount = np.count_nonzero(bagofwords,axis=1) #word count for each text\n",
        "  \n",
        "  #----------------IDF----------------------\n",
        "  _idf = [0 for _ in range(len(textDCount))]\n",
        "  tf_idf= []\n",
        "  for k,value in enumerate(textDCount):\n",
        "    _idf[k] = np.log10(N/value)\n",
        "  _idf = np.array(_idf)\n",
        "  #---------------TF-----------------------\n",
        "  tfDict = [0 for _ in range(n)]\n",
        "  for i in range(n):\n",
        "    count+=1\n",
        "    tfDict[i] = np.array([(text[i]/worddcount[k]) for k,text in enumerate(bagofwords)])\n",
        "    print(\"======================================\")\n",
        "    print(\"Finished\",count,\"st\",\"Calculation\")\n",
        "  \n",
        "  tfDict = np.array(tfDict)\n",
        "  \n",
        "  print(\"Lengths\",len(tfDict),len(tfDict[0]),len(_idf))\n",
        "  \n",
        "  for tf in tfDict:\n",
        "    tf_idf.append(tf.dot(_idf))\n",
        "  mo = []\n",
        "  for value in tf_idf:\n",
        "    mo.append( (np.sum(value)/len(tf_idf))*10**2)\n",
        "  return mo\n",
        "def main():\n",
        "    count=0\n",
        "  \n",
        "   \n",
        "    dicto = []\n",
        "      \n",
        "    dicto = tf_idf(bagofwords)\n",
        "\n",
        "      \n",
        "    return np.array(dicto)\n",
        "if __name__ == '__main__':\n",
        "    temp = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textDCount = np.count_nonzero(bagofwords,axis=0)  #word count of the text body\n",
        "worddcount = np.count_nonzero(bagofwords,axis=1) #word count for each text\n",
        "print(len(textDCount))\n",
        "print(len(worddcount))"
      ],
      "metadata": {
        "id": "mx7B8bO0ES0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}