{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmoCnFci59fb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from math import log2\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTdMg9za6J32"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataMatrix = None\n",
        "        self.labels = None\n",
        "\n",
        "\n",
        "    def getLexicalSize(self,file):\n",
        "        lexiconS = []\n",
        "        with open(file, \"r\") as fp:\n",
        "            for line in fp:\n",
        "                lexiconS.append(line.split()[0])\n",
        "        return  len(lexiconS)\n",
        "    def getLabels(self,data):\n",
        "        self.labels = []\n",
        "        with open(data, \"r\") as fp:\n",
        "            for line in fp:\n",
        "                self.labels.append([int(i) for i in line.split()])\n",
        "\n",
        "    def getData(self,data,lexicon):\n",
        "        bagof = []\n",
        "        with open(data,\"r\") as fp:\n",
        "            for line in fp:\n",
        "                vertex = [0 for i in range(self.getLexicalSize(lexicon))]\n",
        "                array = list(filter(lambda x : x[0] != '<' , line.split()))\n",
        "                for ele in array:\n",
        "                    vertex[int(ele)] +=1\n",
        "                bagof.append(vertex)\n",
        "        return bagof\n",
        "    def MMNorm(self,data,lexicon):\n",
        "        self.dataMatrix = []\n",
        "        minimum = []\n",
        "        maximum = []\n",
        "        # Min Max Classification\n",
        "        for element in self.getData(data,lexicon):\n",
        "            minimum = min(element)\n",
        "            maximum = max(element)\n",
        "            vect = [(i - minimum) / (maximum - minimum) for i in element]\n",
        "            self.dataMatrix.append(vect)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyY2YfnF6Lce"
      },
      "outputs": [],
      "source": [
        "labellex  = Parser()\n",
        "datalex = Parser()\n",
        "\n",
        "testlabel = Parser()\n",
        "testdata = Parser()\n",
        "\n",
        "testlabel.getLabels(\"/content/test-label.dat\")\n",
        "\n",
        "\n",
        "testdata.MMNorm(\"/content/test-data.dat\",\"/content/vocabs.txt\")\n",
        "\n",
        "labellex.getLabels(\"/content/train-label.dat\")\n",
        "\n",
        "\n",
        "datalex.MMNorm(\"/content/train-data.dat\",\"/content/vocabs.txt\")\n",
        "\n",
        "Y = np.array(labellex.labels)\n",
        "X = np.array(datalex.dataMatrix)\n",
        "\n",
        "Y_test = np.array(testlabel.labels)\n",
        "X_test = np.array(testdata.dataMatrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "ABPfelWN7Fno"
      },
      "outputs": [],
      "source": [
        "def get_model(n_inputs, n_outputs,metrics):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_outputs, input_dim=n_inputs,kernel_regularizer=l2(0.9), activation='relu'))\n",
        "    model.add(Dense(n_outputs, input_dim=n_outputs,kernel_regularizer=l2(0.9), activation='relu'))\n",
        "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "    \n",
        "    opt = SGD(learning_rate=0.1,momentum= 0.6)\n",
        "    model.compile(loss='binary_crossentropy',optimizer=opt,metrics=[\"binary_accuracy\",\"mse\",\"binary_crossentropy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(x, y,metrics):\n",
        "    results = list()\n",
        "    bmodel = 0.0\n",
        "    bhist = 0.0\n",
        "    bacc = 0.0\n",
        "    n_inputs, n_outputs = x.shape[1], y.shape[1]\n",
        "    # define evaluation procedure\n",
        "    cv = KFold(n_splits=5)\n",
        "    # enumerate folds\n",
        "    for train_ix, test_ix in cv.split(x):\n",
        "        # prepare data\n",
        "        x_train, x_test = x[train_ix], x[test_ix]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # define model\n",
        "        es = EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=10,min_delta=0.01 )\n",
        "        model = get_model(n_inputs, n_outputs,metrics)\n",
        "        # fit model\n",
        "        \n",
        "        \n",
        "        history = model.fit(x_train, y_train,validation_data=(x_test,y_test),batch_size=66, verbose=1, epochs=500,callbacks=[es])\n",
        "        # make a prediction on the test set\n",
        "        \n",
        "       \n",
        "        scores = model.evaluate(x_test,y_test,verbose=0)\n",
        "        print(scores)\n",
        "        if scores[1] > bacc:\n",
        "   \n",
        "          bmodel = model\n",
        "          bhist = history\n",
        "          bacc = scores[1]\n",
        "    \n",
        "    score = bmodel.evaluate(X_test,Y_test,verbose=0)\n",
        "    plt.plot(bhist.history['loss'], label='train')\n",
        "    plt.plot(bhist.history['val_loss'], label='test')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(score[1],\"\\n\",score[2],\"\\n\",score[3])\n",
        "    return results,history\n",
        "\n",
        "\n",
        "#todo\n",
        "# use the real test data\n",
        "# fix gpu problem\n",
        "\n",
        "# reconsider the acc, mean and std\n",
        "\n",
        "# evaluate model\n",
        "\n",
        "#results1=evaluate_model(X, Y,tf.keras.metrics.MeanSquaredError())\n",
        "results2,history=evaluate_model(X, Y,tf.keras.metrics.BinaryAccuracy())\n",
        "#results3=evaluate_model(X, Y,tf.keras.metrics.BinaryCrossentropy())\n",
        "\n",
        "#Stats(results1)\n",
        "\n",
        "#Stats(results3)\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "\n",
        "\n",
        "# summarize performance"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}